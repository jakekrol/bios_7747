{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes from in-class\n",
    "\n",
    "- For paper presentation: stick at 10m max; \n",
    "- Complete evaluation of course\n",
    "\n",
    "# Notes from \n",
    "- Learn more features\n",
    "- Report accuracy per class\n",
    "- Report global accuracy\n",
    "- Provide training curves with tensor board (for training and validtion)\n",
    "- Provide stopping criteria (e.g., number of epochs)\n",
    "- Classes are imbalanced\n",
    "    - Accuracy may be high\n",
    "    - Use weighted class penalty\n",
    "        - Weight is highest for infrequent classes and weight is lowest for most frequent class\n",
    "Where to put weight: put the weight vector in  the loss function: BCE \n",
    "\n",
    "- If you get stuck at 67%, then only one class is being predicted\n",
    "\n",
    "- Before activation do a batch normalization\n",
    "    - Can be used right before activation\n",
    "    - Helps with vanishing/exploding gradient problem\n",
    "- If overfitting, use dropout\n",
    "    - Use before the last linear layer\n",
    "        - the probability parameter is 1 - p; which indicates the prob of neurons being dropped out\n",
    "- Use double convoluations\n",
    "- Can try different loss functions and optimizers\n",
    "\n",
    "Explain why for batch norm, \n",
    "\n",
    "Image is 28 pixels by 3 channels (RGB)\n",
    "\n",
    "- The whole point of conv is to reduce features. So make sure to reduce the number of features i.e., original (28*28*3)\n",
    "\n",
    "- Look into other CNNs using of using a 3x28x28"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
