{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF -> Adaboost -> Gradient boosted trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF\n",
    "\n",
    "Several weak learners ensembled into majority vote\n",
    "\n",
    "## Adaboost\n",
    "\n",
    "- New tree construction is sensitive to the performance of previous trees\n",
    "- Choose the best performing trees\n",
    "\n",
    "Init first tree\n",
    "\n",
    "- Each sample has equal weight\n",
    "- Amount of say:  $\\frac{1}{2}\\log{\\frac{1-err}{err}}$\n",
    "    - Used as a weigght to measure the tree\n",
    "    - Trees are not independent\n",
    "\n",
    "If a sample is misclassified, the weight of the sample's error is upweighted. $\\exp{amount of say}$\n",
    "\n",
    "## Gradient Boost\n",
    "\n",
    "- Regression: start with $avg(y)$\n",
    "- Classification: start with log odds\n",
    "\n",
    "- Start with weak learner. Shallow tree\n",
    "- Use the graident to update the next tree\n",
    "\n",
    "- Optimize to find the root (where the gradient is zero), using iterative approaches; similar to Newton's method.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
